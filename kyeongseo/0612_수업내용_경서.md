## 이론

### 앙상블의 종류

1. **보팅 (Voting)**

: 여러 개의 다른 종류의 모델이 예측한 결과를 투표 혹은 평균을 통해 최종 선정

- 하드 보팅 (Hard Voting) : 다수결
- 소프트 보팅 (Soft Voting) : 각 확률의 평균
    
    → 일반적으로 하드 보팅보다는 소프트 보팅이 예측 성능이 상대적으로 우수하다.
    
    회귀 모델은 각 모델들이 예측한 수치값의 평균을 통해서 최종 예측
    

1. **베깅 (Bagging)**

: 여러 개의 같은 종류의 모델이 예측한 결과를 투표 혹은 평균을 통해 최종 선정

- Bootstrap 분할 방식 사용
- 랜덤 포레스트
    
    → 여러 개의 결정 트리 모델로 예측한 값을 투표를 통해서 최종 선택하는 베깅의 대표적 모델
    
    - 직관적이어서 결과를 쉽게 이해할 수 있음
    - 다수의 의사결정트리의 의견이 통합되지 않는다면 → 투표에 의한 다수결의 원칙을 따름 → 앙상블 방법
    - 장점 : 실제 값에 대한 추정값 오차 평균화, 분산 감소,
    - 주요 매개변수 (Hyperparameter)
        - 트리의 개수 : `n_estimators`
        - 선택할 특징의 최대 수 : `max_features`
        - 선택할 데이터의 시드 : `random_state`
    - 특징
        - 결정 트리 모델의 과대적합을 통계적 방법으로 해소
        - 결정 트리 모델처럼 쉽고 직관적
        - 부스팅 방식에 비해 빠른 수행 속도
        - 모델 튜닝을 위한 시간이 많이 필요 (하이퍼 파라미터의 종류가 많음)
        - 큰 데이터 세트에도 잘 동작하지만, 트리 개수가 많아질수록 시간이 오래 걸림

1. **부스팅 (Boosting)**

: 여러 개의 같은 종류의 모델이 순차적으로 학습-예측하며 오류를 개선하는 방식

- 여러 개의 모델이 순차적으로 학습-예측하며 잘못 예측한 데이터에 가중치를 부여해 오류를 개선해 나가면서 학습하는 방식 (결정 트리 모형을 베이스로 사용)
- AdaBoost (Adaptive Boosting)
    - RF처럼 의사결정 트리 기반의 모델 → 각각의 트리들이 독립적으로 존재하지 않음
    - 동작 순서
        - 첫 번째 의사결정 트리를 생성 → 위쪽 빨간 원이 3개 있는 곳을 대충 분류 시킴
        - 잘못된 데이터에는 높은 가중치, 맞은 것에는 낮은 가중치 부여
        - 가중치를 부여한 상태에서 다시 분류
- Gradient Boosing Machine
    - AdaBoost와 기본 개념이 동일하고 가중치를 계산하는 방식에서 경사하강법을 이용하여 최적의 가중치(파라미터)를 찾아냄
    - 장단점
        - 학습속도가 느림 (부스팅의 일반적인 단점)
        - 데이터 특성의 스케일을 조정할 필요가 없음 (트리 기반의 특성)
- XG 부스팅 (eXtreme Gradient Boosting)
    - 특징
        - GBM의 단점 : 느림, 과대적합 문제
        - GBM보다 빠름 : Early Stopping 제공
        - 과대적합 방지를 위한 규제 포함
        - 병렬로 빠른 학습이 가능
- Light GBM
    - XG Boosting에 비해 가벼워(Low memory) 속도가 빠른 모델
    - Leaf-wise (수직 방향, 비대칭)로 트리를 성장 시킴 (속도가 빠르다)
    - Level-wise (수평 방향, 깊이 ⬇️, 대칭)보다 오류가 더 적음 (안정성이 올라간다)
    - 장단점
        - 대량(1만개 이상)의 데이터를 병렬로 빠르게 학습 가능 (Low Memory, GPU 활용 가능)
            
            → XG Boosting 대비 2~10배의 속도 (동일 파라미터 설정시)
            
            → 소량의 데이터에서는 제대로 동작하지 않음 (과대적합 위험)
            
        - 예측 속도가 빠름 (Leaf-wise 트리의 장점)
            
            → 그러나 Level-wise에 비해 과적합에 민감
            

## 앙상블 실습

```python
라이브러리 불러오기
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
```

1. **랜덤 포레스트**

```python
1. 모델 생성
forest_model = RandomForestClassifier(n_estimators=100, #트리의 개수
                                      max_features=5, #각각의 트리가 선택할 특성 개수의 최대
                                      max_depth=4, # 각각의 트리의 최대 깊이
                                      random_state=11) # 아무거나 넣어도 됨
                                      
2. 교차검증
result_RF = cross_val_score(forest_model, X_train_one_hot, y_train, cv=5)
result_RF.mean()
```

1. **AdaBoosting**

```python
1. 모델 생성
ada_model = AdaBoostClassifier(n_estimators=100, random_state=14)

2. 교차검증
result_ada = cross_val_score(ada_model, X_train_one_hot, y_train, cv=5)
result_ada.mean()
```

1. **Voting**

```python
1. 모델 생성
voting_model = VotingClassifier(estimators=[('tree', tree_model2),
                                           ('forest', forest_model),
                                           ('ada', ada_model)],
                                            voting='soft')
                                            
2. 교차검증
result_voting = cross_val_score(voting_model, X_train_one_hot, y_train, cv=5)
result_voting.mean()
```

1. **XGBoost**

```python
1. 라이브러리 설치 및 호출
!pip install xgboost
from xgboost import XGBClassifier

2. 모델 생성
xg_model = XGBClassifier(n_estimators=100, random_state=13)

3. 교차검증
result_xg = cross_val_score(xg_model, X_train_one_hot, y_train, cv=5)
result_xg.mean()
```

1. **LightGBM**

```python
1. 라이브러리 설치 및 호출
!pip install lightgbm
from lightgbm import LGBMClassifier

2. 모델 생성
lgb_model = LGBMClassifier(n_estimators=100, random_state=55)

3. 교차검증
result_lgb = cross_val_score(lgb_model, X_train_one_hot, y_train, cv=5)
result_lgb.mean()
```

![image.png](attachment:0c245ff1-4706-478d-ba9b-724c1f534739:image.png)

→ LightGBM은 소량 데이터에서 제대로 동작하지 않아 적절하지 않다.

## Grid Search

: 가장 좋은 설정값(=하이퍼파라미터)으로 찾아주는 자동화 도구

### 1. 데이터 로드 및 분리

```python
1. 라이브러리 호출
from sklearn.neighbors import KNeighborsClassifier # KNN 모델
from sklearn.tree import DecisionTreeClassifier # 의사결정나무 모델

# Grid Search 도구 불러오기
from sklearn.model_selection import GridSearchCV

# 데이터 분할 도구
from sklearn.model_selection import train_test_split

# iris 데이터
from sklearn.datasets import load_iris

2. 데이터 로드
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris['data'], 
																										iris['target'], 
																										test_size=0.3, 
																										random_state=8)

```

### 2. KNN 모델에 그리드서치 적용

```python
param_knn = {'n_neighbors' : range(1,50,1)}
# 딕셔너리 형태
# key값에는 하이퍼파라미터명, value 값에는 찾고자 하는 범위

# GridSearchCV
grid_search_knn = GridSearchCV(KNeighborsClassifier(), #knn모델에서
                              param_knn, #찾고자 하는 하이퍼파라미터와 범위
                              cv=10) # 교차검증할 횟수

grid_search_knn.fit(X_train, y_train) # 최고성능모델이 변수에 저장
```

### 최적의 파라미터값, 점수, 모델명 속성을 통해 확인

```python
print('최적 파라미터 값 : ', grid_search_knn.best_params_)
print('최고 점수 : ', grid_search_knn.best_score_)
print('최고 성능 모델 : ', grid_search_knn.best_estimator_)
```

### 3. 결정트리 모델에 그리드서치 적용

```python
# key값에는 하이퍼파라미터명, value값에는 찾고자하는 그 범위 지정
param_tree = {'max_depth' : range(1,20,2),
             'max_leaf_nodes' : range(10,101,10),
             'min_samples_leaf' : range(20,151,10)}

# GridSearchCV 함수 선언
grid_search_tree = GridSearchCV(DecisionTreeClassifier(), #의사결정나무 모델에서
                              param_tree, #찾고자 하는 하이퍼파라미터와 범위
                              cv=7,
                              verbose=2) #진행상태 표시

grid_search_tree.fit(X_train, y_train)
```

### 최적의 파라미터값, 점수, 모델명 속성을 통해 확인

```python
print('최적 파라미터 값 : ', grid_search_tree.best_params_)
print('최고 점수 : ', grid_search_tree.best_score_)
print('최고 성능 모델 : ', grid_search_tree.best_estimator_)
```

![image.png](attachment:8789676a-c38b-4f07-90bb-9f93f4071b3e:image.png)

test데이터 정확도에 비해 train 데이터의 정확도가 너무 높으면 과대적합 확률 ⬆️

의사결정나무 자체가 과대적합에 빠지기 좋은데, 데이터 수가 적어 과대적합이 나옴

일반적으론 의사결정나무가 정확도가 더 좋지만, 데이터가 너무 소량일 경우 KNN 모델이 더 나을 수 있다.